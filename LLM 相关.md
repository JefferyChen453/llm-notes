### LLM 复读机问题

- 数据偏差：训练语料中存在大量重复文本或高频出现的句子短语
- 训练目标限制：自监督学习通过预测下一词或掩码来学习，训练目标可能使得模型更倾向于生成与输入相似的文本
- 缺乏多样性的训练数据

如何解决？

- 多样性训练数据
- 对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性
- 温度参数：
  - 当温度参数较大时，模型会更加倾向于生成更多的不确定性或随机性较高的文本。这意味着生成的文本可能会更加多样化，可能会包含更多的错误、无意义的词语或者更加奇怪的组合。温度越高，生成的文本就越不可预测，也更容易出现与训练数据不一致的内容。
  - 相反，当温度参数较小时，模型生成的文本会更加保守和可预测，更接近于训练数据中的模式。生成的文本可能会更加准确，但也更加倾向于重复和缺乏创造性。

### 如何选择模型

- BERT：任务是通用的文本处理任务（文本分类、命名实体识别、语义相似度计算），而不依赖于特定领域的知识或语言风格，适用于 NLU 任务。

- LLaMA：LLaMA 包含从 7B 到 65B 的参数范围，训练使用多达14,000亿tokens语料，具有常识推理、问答、数学推理、代码生成、语言理解等能力；由一个Transformer解码器组成。训练预料主要为以英语为主的拉丁语系，不包含中日韩文，适合于英文文本生成的任务

- ChatGLM：ChatGLM是一个面向对话生成的语言模型，适用于构建聊天机器人、智能客服等对话系统。如果你的应用场景需要模型能够生成连贯、流畅的对话回复，并且需要处理对话上下文、生成多轮对话等，ChatGLM模型可能是一个较好的选择。ChatGLM的架构为Prefix decoder，训练语料为中英双语，中英文比例为1:1。所以适合于中文和英文文本生成的任务。

### 如何处理长文本

- 分块处理：长文本分成小片段，可以使用重叠来保持上下文一致性
- 层次建模：将长文本划分为更小的单元。例如，可以将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理

### SFT 之后模型变傻

- 数据偏移：SFT过程中使用的微调数据集可能与基座模型在预训练阶段接触到的数据分布有所不同。如果微调数据集与预训练数据集之间存在显著的差异，模型可能会在新任务上表现较差
- 非典型标注：微调数据集的标注可能存在错误或不准确的标签。这些错误的标签可能会对模型的性能产生负面影响
- 过拟合：如果微调数据集相对较小，或者模型的容量（参数数量）较大，模型可能会过拟合微调数据，导致在新的输入上表现不佳
- 缺乏多样性：微调数据集可能缺乏多样性，未能涵盖模型在新任务上可能遇到的各种输入情况。这可能导致模型在面对新的、与微调数据集不同的输入时出现困惑或错误的预测

如何解决？

- 正则化技术（如权重衰减、dropout）来减少过拟合
- 数据增强，通过对微调数据进行一些变换或扩充来增加多样性

### 预训练和微调哪个阶段注入知识的？

知识注入是在预训练阶段进行的，预训练模型通过大规模通用数据的训练，学习到了丰富的语言知识和表示能力，预训练的目标通常是通过自我预测任务，例如掩码语言模型（Masked Language Model，MLM）或下一句预测（Next Sentence Prediction，NSP），为后续的微调阶段提供了基础。微调阶段则是在预训练模型的基础上，使用特定任务（NER，text classification）的数据进行进一步训练和调整，以提升性能。

### 多轮对话任务如何微调模型？

- 数据准备：收集或生成于目标对话任务相关的数据集，数据集应该包含多轮对话的对话历史、当前对话回合的输入、和对应回答
- 模型选择：用 GPT GLM 等模型作为基础模型
- 添加任务特定层：为了适应多轮对话任务，需要在预训练模型上添加一些任务特定的层。这些层可以用于处理对话历史、上下文理解和生成回答等任务相关的操作
- 微调过程：使用多轮对话数据集对预训练模型进行微调。微调的过程类似于监督学习，通过最小化模型在训练集上的损失函数来优化模型参数
- 超参数调整：微调过程中需要选择合适的学习率、批次大小、训练轮数等超参数。可以通过交叉验证或其他调参方法来选择最佳的超参数组合
- 评估调优：计算准确率、召回率、F1分数等
- 推理和部署

### 灾难性遗忘问题

模型微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能下降。常见于迁移学习或连续学习中。

原因

- 数据分布差异：新任务与预训练数据分布差异大，模型过度适应新任务导致旧任务性能下降
- 参数更新冲突：新任务的梯度更新与旧任务发生冲突，导致旧任务的知识被遗忘

解决

- 重播缓冲区（Replay Buffer）：在微调过程中，使用一个缓冲区来存储旧任务的样本，然后将旧任务的样本与新任务的样本一起用于训练。这样可以保留旧任务的知识，减少灾难性遗忘的发生
- 弹性权重共享（Elastic Weight Consolidation）：通过引入正则化项，限制模型参数的变动范围，以保护之前学习到的知识
- 增量学习（Incremental Learning）：将微调过程分为多个阶段，每个阶段只微调一小部分参数。这样可以逐步引入新任务，减少参数更新的冲突，降低灾难性遗忘的风险
- 多任务学习（Multi-Task Learning）：在微调过程中，同时训练多个相关任务，以提高模型的泛化能力和抗遗忘能力。通过共享模型参数，可以在不同任务之间传递知识，减少灾难性遗忘的影响。