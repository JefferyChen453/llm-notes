# 2	概述

## 2.1	背景

### 扩展法则：性能与模型规模、数据量、计算量的关系

- KM
- Chinchilla

### 涌现能力

- 上下文学习（ICL）：为模型提供一个指令或任务的演示，根据输入生成预期结果而无需单独训练。GPT-1 2无，3有。

- 指令遵循：先进行指令微调，LLM 就能在训练中未出现的指令上也有很好的效果
- 逐步推理：利用 CoT 提示策略，让 LLM 在算术推理基准任务上的性能提高

### LLM中的关键技术

- 扩展：加大规模

- 训练
- 能力引导：有些潜在能力不会显示展现。要依靠指令、ICL、CoT提示策略展现出来
- 对齐微调：让 LLM 与人类价值观保持一致。``InstructGPT [61] 设计了一种有效的微调方法，使 LLM 能够
  按照期望的指令进行操作``
- 工具操作：让 LLM 利用 tools（如计算器、搜索引擎）



## 2.2	GPT系列

### 早期探索

- GPT-1：无监督预训练和有监督微调的混合方法；确立了对自然语言文本进行建模的基本原则，即预
  测下一个单词

- GPT-2：架构类似，参数规模 1.5B；无监督语言建模来执行任务，无需使用标记数据进行显式微调

### 能力飞跃

- GPT-3：参数规模175B；提出 ICL：预测正确的任务解决方案，该解决方案可以被格式化为给定任务描述和示范下的文本序列

  ``pretrain范式：预测给定上下文条件下的后续文本序列``

### 能力增强

- 用代码数据进行训练：Codex-在 github 代码上微调的 GPT

- 与人类对齐：近端策略优化（Proximal Policy Optimization, PPO）--> InstructGPT  提出 RLHF ``OpenAI 的论文和文档中似乎很少使用 “指令微调” 一词，而是用在人类示例上有监督微调来替代（即 RLHF 算法的第一步）``

# 3	LLM 资源

## 3.1	公开 checkpoints



## 3.2	常用语料库

- Books
- CommonCrawl
- Reddit Links
- Wikipedia
- Code
- The Pile
- GPT-3 (175B) 训练：CommonCrawl、WebText2 、Books1 、Books2、Wikipedia

- PaLM (540B) 训练：社交媒体对话、过滤后的网页、书籍、Github、多语言维基百科和新闻组成的预训练数据集
- LLaMA (6B / 13B) 训练：CommonCrawl、C4、Github、Wikipedia、书籍、ArXiv、StackExchange



## 3.3	代码库

- Transformers
- ...



# 4	预训练

## 4.1	数据收集

### 4.1.1	数据来源

- 通用文本：网页、对话、书籍
- 专用文本：多语言、科学文本
- 代码

### 4.1.2	数据预处理

- 质量过滤

  - 基于分类器：使用高质量文本训练选择分类器，识别过滤低质量数据 ``可能会删除方言、口语和社会语言的高质量文本``

  - 基于启发式：

    - 基于语言过滤：若 LLM 主要用于某项语言，删除其他语言文本

    - 基于度量过滤：基于 metrics 如困惑度，检测删除不自然的句子
    - 基于统计过滤：利用语料库的统计特征，例如标点符号分布、符号与单词比率和句子长度，来衡量文本质量并过滤低质量数据。
    - 基于关键词过滤：例如 HTML 标签、超链接、模板和攻击性词语

  

- 去重：``重复数据会降低语言模型的多样性，可能导致训练过程不稳定。三个粒度（句子级、文档级和数据集级）``

- 隐私去除
- 分词

### 4.1.3	与训练数据对 LLM 影响

- 混合来源
- 数据数量
- 数据质量：重复的数据会降低 LLM 从上下文中复制的能力



## 4.2	架构

### 4.2.1	主流架构

- Encoder-Decoder: T5、BART
- Causal-Decoder: GPT、BLOOM、Gopher、LLaMA

- Prefix-Decoder (也称非因果 Decoder): 修正了因果解码器的掩码机制，以使其能够对前缀 token 执行双向注意力代表工作：GLM-130B、U-PaLM

``可以考虑通过 MoE 混合上述架构``

### 4.2.2	详细配置

- 标准化
  - LN：最初 Transformer 中使用 post-LN，大多数 LLM 会用 pre-LN 实现更稳定训练，但会带来一定性能损失
  - RMSNorm：训练速度和性能方面优越性 Gopher
  - DeepNorm：训练稳定性 和 post-Norm 一起被用在 GLM-130B 中
  - Embedding 之后加额外的 LN 可以稳定训练，但性能显著下降
- 激活函数
  - GeLU
  - GLU、SwiGLU、GeGLU
- 位置编码
  - 正弦函数（绝对）
  - 学习的位置编码（绝对）：LLM 中使用更多
  - 相对位置编码：根据键和查询之间的偏移量生成嵌入；可以在比训练序列更长的序列上表现良好
  - 通过基于绝对位置设置特定的旋转矩阵，RoPE 中的键和查询之间的分数可以使用相对位置信息计算

- 注意力
  - Attention
  - Flash Attention：高效建模更长序列
  - Sparse Attention：GPT-3 更低计算复杂度
  - 偏置：大多 LLM 在每个 Linear 和 LN 中保留了偏置。在 PaLM、Galactica 中被移除，认为可以增强稳定行

``综合上述讨论，我们总结了现有文献中的详细配置建议。为了有更强的泛化能力和训练稳定性，建议选择前置的 RMS 进行层标准化，并选择 SwiGLU 或 GeGLU 作为激活函数。此外，在位置编码方面，RoPE 或 ALiBi 是更好的选择，因为它们在长序列上表现更好。``

### 4.2.3	预训练任务

- 语言建模（LM）：基于序列中前面的 token x<i，自回归地预测目标 token xi
- 去噪自编码（DAE）：输入中有一些被随机替换区间的损坏文本，训练语言模型以恢复被替换的 tokens    代表模型：GLM-130B、T5

``总结：Causal-Decoder 有更好的 zero/few-shot 能力；广泛观察到扩展法则``



## 4.3	模型训练

### 4.3.1	优化设置

- 批量：GPT-3、PaLM 引入动态 batch_size，可稳定 LLM 训练过程
- 学习率
  - warm-up：训练初始的 0.1%~0.5% 的步骤中，线性增加 lr 到最大值 [5e-5, 1e-4]。GPT-3 的 lr 为 6e-5
  - decay：衰减至最大值的 10%
- 优化器：Adam AdamW
- 稳定训练：weight decay、梯度裁剪。解决 loss 突增：PaLM 和 OPT 从发生徒增前一个 checkpoint 开始重新训练；GLM 发现 embedding 异常梯度通常导致徒增，提出减少 embed 维度缓解

### 4.3.2	可扩展的训练技术

- 3D 并行
  - 数据并行
  - 流水线并行
  - 张量并行
- ZeRO：优化器状态分区、梯度分区和参数分区。PyTorch 中实现的 FSDP 和 ZeRO 类似。
- 混合精度训练：FP32 (BERT) --> FP16 (会导致精度损失) --> BF16 (更多指数位，更少有效位)
- 整体训练建议



# 5	LLM 适配微调

## 5.1	指令微调

构建指令格式 --> 有监督微调

### 5.1.1	格式化实例构建	（Fig. 5）

包括：任务描述（即指令）--> 一对输入输出 --> 少量示例（可选）

- 格式化已有数据集：使用人类撰写的任务描述来增广带标注的数据集，这些描述通过解释任务目标来指导 LLM 理解任务。例如，在图 5（b）中，每个问答任务的实例都添加了一个任务描述“请回答下列问题”
  
- 格式化人类需求：丰富任务多样化，包括开放式生成、开放式问答、头脑风暴和聊天等
- 构建实例的关键因素
  - 增加指令：扩大任务数量可以提高 LLM 泛化能力
  - 设计格式：添加任务描述和示例可以产生实质性改进；而将避免、原因、建议添加到指令会产生不利影响；为了引出推理能力，一些研究建议将面向数据集的 CoT 实例（如算术推理）。同时使用包含和不包含 CoT 的样本在下游任务中可以获得较好性能。

​	``指令多样性 > 实例数量``

### 5.1.2	指令微调策略

优化过程与预训练有一些不同 [64]，比如训练目标函数（如序列到序列的损失）和优化参数设置（如更小的批量大小和学习率）

- 平衡数据分布：指令微调涉及到多任务混合
  - 按比例采样每种实例
- 结合指令微调和预训练
  - OPT-IML 在指令微调中加入与训练数据，可以看作对模型的正则化
  - 不采用 pretain -> 指令微调 分阶段，而是直接将预训练数据和指令微调数据混合，从头训练模型 如 GLM-130B

### 5.1.3	微调效果

- 性能改进
- 泛化性



## 5.2	对齐微调

### 5.2.1	对齐的标准

- 有用性、诚实性、无害性

### 5.2.2	人类反馈的收集

- 标注人员
- 人类反馈的收集
  - 基于排序
  - 基于问题
  - 基于规则

### 5.2.3	RLHF

三个组件：待对齐的 PLM、奖励模型 RM、RL 算法

- RLHF 系统
  - 第一个主流 RLHF 模型 InstructGPT 使用了 GPT-3 作为 PLM
  - RM 提供指导信号，反映人对 LM 生成的文本的偏好，以标量表示。
    - RM 的两种形式：经过微调的 LM 或使用人类偏好数据重新训练的 LM
    - RM 通常采用与待对齐的 PLM 参数尺度不同的奖励模型：如 OpenAI 使用 6B GPT-3；DeepMind 使用 7B Gopher
  - 用 RL 算法用于微调，常用 PPO

- RLHF 关键步骤
  - 监督微调：需要一个包含 指令+所需输出 的数据集，对 PLM 先微调
  - 训练奖励模型：使用 人类反馈数据集 训练 RM。
    - 向 PLM 输入提示，生成一定数量文本
    - 让人工标注员为 输入-输出对 标注偏好（用排序标注减小不同标注者偏好差异）
    - 训练 RM 来预测人类偏好输出
  - RL 算法微调：RL 问题的策略（policy）由 PLM 给出（将提示作为输入并返回输出文本），行动空间（action space）是 LM 的词表，状态（state）是目前生成的token 序列，奖励（reward）则由 RM 提供



## 5.3	高效微调

### 5.3.1	Parameter-Efficient

- Adapter Tuning：bottle-neck 架构串行加入 Transformer 层
- Prefix Tuning：在 Transformer 层前面添加具有任务特异性的 prefix 向量

- Prompt Tuning：在输入层添加可训练的提示向量
- LoRA：近似每层的更新矩阵

![image-20240415112527264](C:\Users\Jeffery Chen\AppData\Roaming\Typora\typora-user-images\image-20240415112527264.png)

### 5.3.2	LLM 上的参数高效微调



# 6	使用

## 6.1	ICL

### 6.1.1	形式

![image-20240415113139874](C:\Users\Jeffery Chen\AppData\Roaming\Typora\typora-user-images\image-20240415113139874.png)

**``ICL 还与指令微调（在5.1中已讨论）有着密切的联系，因为它们都将任务或样例转化为自然语言的形式。然而，指令微调需要微调 LLM 来增强适配，而 ICL 仅仅是以提示的方式来使用 LLM。此外，指令微调可以提高 LLM 执行目标任务的 ICL 能力，尤其是在零样本设置时（仅使用任务描述）``**

### 6.1.2	示范设计

- 示范选择：如何选择 ICL 中的示范样例
  - 启发式方法：用 k-NN 匹配语义相近的样例
  - 基于 LLM：根据添加样例后的性能提升评估每个样例的信息量

- 示范格式：选择示例后，将其整合及格式化为对 LLM 的自然语言提示

  - 用相应的输入-输出对
  - 添加任务描述 / CoT 提示

  ``两种生成高质量示范格式的方法：Auto-CoT 利用 LLM 使用零样本提示生成中间推理；least-to-most 首先询问 LLM 来执行问题分解，然后利用 LLM 根据先前解决的中间答案依次解决子问题``

- 示范顺序：LLM 有时会收到顺序偏差影响，倾向于重复示范结尾附近的答案。
  - 启发式：查询相似度，越高的离结尾放越近

### 6.1.3	底层机制

- 预训练如何影响 ICL？
  - 训练任务的设计
  - 训练语料的来源而非规模
  - 训练数据的分布：当训练数据可以被聚类成为许多不常见类别，而不是均匀分布，模型会表现出 ICL 能力
- LLM 如何实现 ICL？
  - 通过前向计算，LLM 生成关于示范的元梯度，并通过注意力机制隐式地执行梯度下降



## 6.2	CoT

一种改进的提示策略

### 6.2.1	使用 CoT 的 ICL

- few-shot CoT：ICL 的一个特例，将每个示范 ⟨ 输入，输出 ⟩ 扩充为 ⟨ 输入，CoT，输出 ⟩，在 zero/few-shot 上和 ICL 配合使用
  - 对每个问题使用多个 CoT
  - Auto-CoT：利用 zero-shot CoT，通过特别提示 LLM 生成 CoT
    - 为提高性能，Auto-CoT 进一步将训练集中的问题分成不同的簇，并选择最接近每个簇质心的问题，它们应该可以很好地代表整个训练集中的问题
  - 增强的 CoT：如何生成多个推理路径，并在其中寻找一致性
    - self-consistency：用 LLM 生成多个推理路径，然后对所有答案进行集成 （例如通过在这些路径中进行投票来选择最一致的答案）
- zero-shot CoT：用“Let’s think step by step”提示 LLM 来生成推理步骤，然后通过用“Therefore, the answer is”提示来得出最终答案。他们发现，这种策略在模型规模超过一定大小时可以显著提高性能，但在小规模的模型中效果不佳，这是涌现能力的重要表现。

### 6.2.2	进一步讨论

- CoT 何时适用于 LLM？
  - CoT 是一种涌现能力，只适用于大模型
  - 效果主要体现在需要推理的任务；对于不复杂的任务，CoT 会比标准提示更差
- LLM 为什么能进行 CoT 推理？
  - 思维能力来源：？
  - 提示中组成部分的影响：符号（symbols）（例如算术推理中的数值量）、模式（patterns）（例如算术推理中的方程）和文本（text）



# 7	能力评测

## 7.1	基础评测任务

### 7.1.1	语言生成

- 语言建模：即基于之前 token 预测下一个 token 的能力
- 条件文本生成：机器翻译、文本摘要、问答系统。评分基于准确率、BLEU、ROUGE
- 代码合成
- 主要问题
  - 可控生成：全局关系难以捕捉
  - 专业化生成：在整合新旧知识发生灾难性遗忘、ICL 能力下降

### 7.1.2	知识利用

- 闭卷问答

- 开卷问答

- 知识补全：知识图谱补全、事实补全。InstructGPT 表明指令微调有助于补全。

- 主要问题

  - 幻觉：生成的信息与现有来源冲突（**内在幻觉**），或无法通过现有来源验证（**外在幻觉**）

  - 知识实时性

### 7.1.3	复杂推理

- 知识推理
- 符号推理：在形式化规则设定中操作符号以实现某些特定目标
- **数学推理**
  - 数学问题求解数据集：SVAMP [274]、GSM8k [273] 和MATH [312] 
  - 数学问题证明
- 主要问题
  - 不一致性：错误的推理仍生成正确答案，或正确推理生成错误答案



## 7.2	高级能力评估

### 7.2.1	与人类对齐

### 7.2.2	与外部环境互动

### 7.2.1	工具使用







































