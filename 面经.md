### 20240507	面壁智能	1h

1. 项目+八股穿插

- LLaMA-2、GPT、GLM 细问，模型架构、位置编码、Norm、预训练任务上的差异 **（LLaMA-3 问了但没看）**
- Pre-Norm、Post-Norm、LayerNorm、RMSNorm、**DeepNorm（没想起来）**各自的优缺点
- BERT、Transformer 一条龙：RoBERTa、**DeBERTa（解耦注意力实现忘了）**、RoPE、MQA & GQA、**Attention 改进（Longformer、Flash Attention）**
- DeepSpeed、FSDP、**Accelerate（了解不多）**之间的关系以及 ZeRO 三个 stage

2. 手搓

- 链表每个元素移动 k 个位置 （力扣 No.61）
- 手写多头自注意力+缩放点积